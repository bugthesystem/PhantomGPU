[
  {
    "gpu_name": "Tesla V100",
    "gpu_architecture": "Volta",
    "model_name": "ResNet-50",
    "model_type": "CNN",
    "batch_sizes": [1, 8, 16, 32, 64],
    "measurements": [
      {
        "batch_size": 1,
        "precision": "FP32",
        "inference_time_ms": 1.4,
        "memory_usage_mb": 200.0,
        "throughput_samples_per_sec": 714.0,
        "gpu_utilization_percent": 65.0,
        "power_usage_watts": 250.0,
        "temperature_celsius": 75.0,
        "runs": 1000,
        "std_dev_ms": 0.05
      },
      {
        "batch_size": 8,
        "precision": "FP32",
        "inference_time_ms": 8.2,
        "memory_usage_mb": 850.0,
        "throughput_samples_per_sec": 975.0,
        "gpu_utilization_percent": 85.0,
        "power_usage_watts": 280.0,
        "temperature_celsius": 78.0,
        "runs": 1000,
        "std_dev_ms": 0.15
      },
      {
        "batch_size": 16,
        "precision": "FP32",
        "inference_time_ms": 15.8,
        "memory_usage_mb": 1650.0,
        "throughput_samples_per_sec": 1013.0,
        "gpu_utilization_percent": 92.0,
        "power_usage_watts": 295.0,
        "temperature_celsius": 80.0,
        "runs": 1000,
        "std_dev_ms": 0.25
      }
    ],
    "system_info": {
      "driver_version": "460.32.03",
      "cuda_version": "11.2",
      "cpu_model": "Intel Xeon Silver 4216",
      "memory_gb": 64.0,
      "pcie_generation": "PCIe 3.0"
    },
    "timestamp": "2024-01-15T10:30:00Z",
    "source": "MLPerf Inference v1.1"
  },
  {
    "gpu_name": "Tesla V100",
    "gpu_architecture": "Volta",
    "model_name": "BERT-Base",
    "model_type": "Transformer",
    "batch_sizes": [1, 8, 16, 32],
    "measurements": [
      {
        "batch_size": 1,
        "precision": "FP32",
        "inference_time_ms": 4.2,
        "memory_usage_mb": 520.0,
        "throughput_samples_per_sec": 238.0,
        "gpu_utilization_percent": 72.0,
        "power_usage_watts": 245.0,
        "temperature_celsius": 73.0,
        "runs": 1000,
        "std_dev_ms": 0.12
      },
      {
        "batch_size": 8,
        "precision": "FP16",
        "inference_time_ms": 26.8,
        "memory_usage_mb": 2400.0,
        "throughput_samples_per_sec": 298.0,
        "gpu_utilization_percent": 88.0,
        "power_usage_watts": 275.0,
        "temperature_celsius": 77.0,
        "runs": 1000,
        "std_dev_ms": 0.45
      },
      {
        "batch_size": 16,
        "precision": "FP16",
        "inference_time_ms": 48.5,
        "memory_usage_mb": 4200.0,
        "throughput_samples_per_sec": 330.0,
        "gpu_utilization_percent": 92.0,
        "power_usage_watts": 285.0,
        "temperature_celsius": 79.0,
        "runs": 1000,
        "std_dev_ms": 0.85
      },
      {
        "batch_size": 32,
        "precision": "FP32",
        "inference_time_ms": 125.6,
        "memory_usage_mb": 7800.0,
        "throughput_samples_per_sec": 254.0,
        "gpu_utilization_percent": 95.0,
        "power_usage_watts": 295.0,
        "temperature_celsius": 81.0,
        "runs": 500,
        "std_dev_ms": 2.1
      }
    ],
    "system_info": {
      "driver_version": "460.32.03",
      "cuda_version": "11.2",
      "cpu_model": "Intel Xeon Silver 4216",
      "memory_gb": 64.0,
      "pcie_generation": "PCIe 3.0"
    },
    "timestamp": "2024-01-16T11:15:00Z",
    "source": "MLPerf Inference v1.1"
  },
  {
    "gpu_name": "Tesla V100",
    "gpu_architecture": "Volta",
    "model_name": "YOLO v8",
    "model_type": "CNN",
    "batch_sizes": [1, 4, 8, 16],
    "measurements": [
      {
        "batch_size": 1,
        "precision": "FP32",
        "inference_time_ms": 12.5,
        "memory_usage_mb": 1200.0,
        "throughput_samples_per_sec": 80.0,
        "gpu_utilization_percent": 78.0,
        "power_usage_watts": 260.0,
        "temperature_celsius": 76.0,
        "runs": 1000,
        "std_dev_ms": 0.3
      },
      {
        "batch_size": 4,
        "precision": "FP16",
        "inference_time_ms": 38.2,
        "memory_usage_mb": 3800.0,
        "throughput_samples_per_sec": 104.0,
        "gpu_utilization_percent": 87.0,
        "power_usage_watts": 278.0,
        "temperature_celsius": 78.0,
        "runs": 1000,
        "std_dev_ms": 0.8
      },
      {
        "batch_size": 8,
        "precision": "FP16",
        "inference_time_ms": 72.1,
        "memory_usage_mb": 6400.0,
        "throughput_samples_per_sec": 111.0,
        "gpu_utilization_percent": 90.0,
        "power_usage_watts": 285.0,
        "temperature_celsius": 80.0,
        "runs": 1000,
        "std_dev_ms": 1.2
      },
      {
        "batch_size": 16,
        "precision": "INT8",
        "inference_time_ms": 95.8,
        "memory_usage_mb": 8900.0,
        "throughput_samples_per_sec": 167.0,
        "gpu_utilization_percent": 93.0,
        "power_usage_watts": 290.0,
        "temperature_celsius": 81.0,
        "runs": 500,
        "std_dev_ms": 1.8
      }
    ],
    "system_info": {
      "driver_version": "460.32.03",
      "cuda_version": "11.2",
      "cpu_model": "Intel Xeon Silver 4216",
      "memory_gb": 64.0,
      "pcie_generation": "PCIe 3.0"
    },
    "timestamp": "2024-01-17T13:30:00Z",
    "source": "Community Benchmarks"
  },
  {
    "gpu_name": "A100",
    "gpu_architecture": "Ampere",
    "model_name": "BERT-Base",
    "model_type": "Transformer",
    "batch_sizes": [1, 8, 16, 32, 64],
    "measurements": [
      {
        "batch_size": 1,
        "precision": "FP32",
        "inference_time_ms": 2.8,
        "memory_usage_mb": 450.0,
        "throughput_samples_per_sec": 357.0,
        "gpu_utilization_percent": 78.0,
        "power_usage_watts": 220.0,
        "temperature_celsius": 65.0,
        "runs": 1000,
        "std_dev_ms": 0.08
      },
      {
        "batch_size": 8,
        "precision": "FP32",
        "inference_time_ms": 18.5,
        "memory_usage_mb": 2100.0,
        "throughput_samples_per_sec": 432.0,
        "gpu_utilization_percent": 88.0,
        "power_usage_watts": 265.0,
        "temperature_celsius": 70.0,
        "runs": 1000,
        "std_dev_ms": 0.22
      },
      {
        "batch_size": 16,
        "precision": "FP16",
        "inference_time_ms": 24.2,
        "memory_usage_mb": 3200.0,
        "throughput_samples_per_sec": 661.0,
        "gpu_utilization_percent": 94.0,
        "power_usage_watts": 285.0,
        "temperature_celsius": 72.0,
        "runs": 1000,
        "std_dev_ms": 0.35
      }
    ],
    "system_info": {
      "driver_version": "470.57.02",
      "cuda_version": "11.4",
      "cpu_model": "AMD EPYC 7742",
      "memory_gb": 128.0,
      "pcie_generation": "PCIe 4.0"
    },
    "timestamp": "2024-02-10T14:20:00Z",
    "source": "NVIDIA Technical Blog"
  },
  {
    "gpu_name": "RTX 4090",
    "gpu_architecture": "Ada Lovelace",
    "model_name": "Stable Diffusion",
    "model_type": "GAN",
    "batch_sizes": [1, 4, 8],
    "measurements": [
      {
        "batch_size": 1,
        "precision": "FP16",
        "inference_time_ms": 1250.0,
        "memory_usage_mb": 8500.0,
        "throughput_samples_per_sec": 0.8,
        "gpu_utilization_percent": 98.0,
        "power_usage_watts": 420.0,
        "temperature_celsius": 75.0,
        "runs": 100,
        "std_dev_ms": 25.0
      },
      {
        "batch_size": 4,
        "precision": "FP16",
        "inference_time_ms": 4800.0,
        "memory_usage_mb": 18500.0,
        "throughput_samples_per_sec": 0.83,
        "gpu_utilization_percent": 99.0,
        "power_usage_watts": 435.0,
        "temperature_celsius": 78.0,
        "runs": 100,
        "std_dev_ms": 85.0
      },
      {
        "batch_size": 8,
        "precision": "FP16",
        "inference_time_ms": 9200.0,
        "memory_usage_mb": 22800.0,
        "throughput_samples_per_sec": 0.87,
        "gpu_utilization_percent": 99.5,
        "power_usage_watts": 445.0,
        "temperature_celsius": 80.0,
        "runs": 100,
        "std_dev_ms": 120.0
      },
      {
        "batch_size": 2,
        "precision": "FP16",
        "inference_time_ms": 2350.0,
        "memory_usage_mb": 12200.0,
        "throughput_samples_per_sec": 0.85,
        "gpu_utilization_percent": 98.5,
        "power_usage_watts": 428.0,
        "temperature_celsius": 76.0,
        "runs": 100,
        "std_dev_ms": 45.0
      },
      {
        "batch_size": 1,
        "precision": "FP32",
        "inference_time_ms": 2100.0,
        "memory_usage_mb": 15800.0,
        "throughput_samples_per_sec": 0.48,
        "gpu_utilization_percent": 96.0,
        "power_usage_watts": 415.0,
        "temperature_celsius": 74.0,
        "runs": 100,
        "std_dev_ms": 55.0
      }
    ],
    "system_info": {
      "driver_version": "522.25",
      "cuda_version": "11.8",
      "cpu_model": "Intel Core i9-13900K",
      "memory_gb": 32.0,
      "pcie_generation": "PCIe 4.0"
    },
    "timestamp": "2024-03-05T09:45:00Z",
    "source": "Community Benchmarks"
  },
  {
    "gpu_name": "RTX 4090",
    "gpu_architecture": "Ada Lovelace", 
    "model_name": "Stable Diffusion XL",
    "model_type": "GAN",
    "batch_sizes": [1, 2, 4],
    "measurements": [
      {
        "batch_size": 1,
        "precision": "FP16",
        "inference_time_ms": 2850.0,
        "memory_usage_mb": 11200.0,
        "throughput_samples_per_sec": 0.35,
        "gpu_utilization_percent": 99.0,
        "power_usage_watts": 440.0,
        "temperature_celsius": 77.0,
        "runs": 100,
        "std_dev_ms": 65.0
      },
      {
        "batch_size": 2,
        "precision": "FP16",
        "inference_time_ms": 5400.0,
        "memory_usage_mb": 18600.0,
        "throughput_samples_per_sec": 0.37,
        "gpu_utilization_percent": 99.5,
        "power_usage_watts": 448.0,
        "temperature_celsius": 79.0,
        "runs": 100,
        "std_dev_ms": 95.0
      },
      {
        "batch_size": 4,
        "precision": "FP16",
        "inference_time_ms": 10200.0,
        "memory_usage_mb": 23500.0,
        "throughput_samples_per_sec": 0.39,
        "gpu_utilization_percent": 99.8,
        "power_usage_watts": 452.0,
        "temperature_celsius": 81.0,
        "runs": 100,
        "std_dev_ms": 145.0
      }
    ],
    "system_info": {
      "driver_version": "537.13",
      "cuda_version": "12.1",
      "cpu_model": "Intel Core i9-13900K",
      "memory_gb": 32.0,
      "pcie_generation": "PCIe 4.0"
    },
    "timestamp": "2024-08-15T14:30:00Z",
    "source": "Community Benchmarks - SDXL"
  },
  {
    "gpu_name": "RTX 4090",
    "gpu_architecture": "Ada Lovelace",
    "model_name": "YOLO v8",
    "model_type": "CNN",
    "batch_sizes": [1, 4, 8, 16],
    "measurements": [
      {
        "batch_size": 1,
        "precision": "FP16",
        "inference_time_ms": 8.5,
        "memory_usage_mb": 1200.0,
        "throughput_samples_per_sec": 117.6,
        "gpu_utilization_percent": 85.0,
        "power_usage_watts": 280.0,
        "temperature_celsius": 65.0,
        "runs": 1000,
        "std_dev_ms": 0.8
      },
      {
        "batch_size": 4,
        "precision": "FP16",
        "inference_time_ms": 22.0,
        "memory_usage_mb": 2800.0,
        "throughput_samples_per_sec": 181.8,
        "gpu_utilization_percent": 92.0,
        "power_usage_watts": 320.0,
        "temperature_celsius": 68.0,
        "runs": 1000,
        "std_dev_ms": 1.2
      },
      {
        "batch_size": 8,
        "precision": "FP16",
        "inference_time_ms": 38.5,
        "memory_usage_mb": 4200.0,
        "throughput_samples_per_sec": 207.8,
        "gpu_utilization_percent": 95.0,
        "power_usage_watts": 350.0,
        "temperature_celsius": 70.0,
        "runs": 1000,
        "std_dev_ms": 1.8
      },
      {
        "batch_size": 16,
        "precision": "FP16",
        "inference_time_ms": 68.2,
        "memory_usage_mb": 6800.0,
        "throughput_samples_per_sec": 234.6,
        "gpu_utilization_percent": 97.0,
        "power_usage_watts": 375.0,
        "temperature_celsius": 72.0,
        "runs": 1000,
        "std_dev_ms": 2.5
      }
    ],
    "system_info": {
      "driver_version": "537.13",
      "cuda_version": "12.1",
      "cpu_model": "Intel Core i9-13900K",
      "memory_gb": 32.0,
      "pcie_generation": "PCIe 4.0"
    },
    "timestamp": "2024-09-20T11:15:00Z",
    "source": "YOLOv8 Official Benchmarks"
  },
  {
    "gpu_name": "Tesla V100",
    "gpu_architecture": "Volta",
    "model_name": "LLaMA 2 7B",
    "model_type": "LLM",
    "batch_sizes": [1, 2, 4, 8],
    "measurements": [
      {
        "batch_size": 1,
        "precision": "FP32",
        "inference_time_ms": 45.2,
        "memory_usage_mb": 14800.0,
        "throughput_samples_per_sec": 22.1,
        "gpu_utilization_percent": 88.0,
        "power_usage_watts": 275.0,
        "temperature_celsius": 78.0,
        "runs": 500,
        "std_dev_ms": 1.8
      },
      {
        "batch_size": 2,
        "precision": "FP16",
        "inference_time_ms": 72.5,
        "memory_usage_mb": 15200.0,
        "throughput_samples_per_sec": 27.6,
        "gpu_utilization_percent": 92.0,
        "power_usage_watts": 285.0,
        "temperature_celsius": 80.0,
        "runs": 500,
        "std_dev_ms": 2.9
      },
      {
        "batch_size": 4,
        "precision": "FP16",
        "inference_time_ms": 138.7,
        "memory_usage_mb": 15900.0,
        "throughput_samples_per_sec": 28.8,
        "gpu_utilization_percent": 94.0,
        "power_usage_watts": 290.0,
        "temperature_celsius": 81.0,
        "runs": 500,
        "std_dev_ms": 4.2
      },
      {
        "batch_size": 8,
        "precision": "FP16",
        "inference_time_ms": 265.3,
        "memory_usage_mb": 16800.0,
        "throughput_samples_per_sec": 30.1,
        "gpu_utilization_percent": 96.0,
        "power_usage_watts": 295.0,
        "temperature_celsius": 82.0,
        "runs": 300,
        "std_dev_ms": 6.8
      }
    ],
    "system_info": {
      "driver_version": "460.32.03",
      "cuda_version": "11.2",
      "cpu_model": "Intel Xeon Silver 4216",
      "memory_gb": 64.0,
      "pcie_generation": "PCIe 3.0"
    },
    "timestamp": "2024-11-15T16:20:00Z",
    "source": "LLaMA 2 Community Benchmarks"
  },
  {
    "gpu_name": "A100",
    "gpu_architecture": "Ampere",
    "model_name": "LLaMA 2 7B",
    "model_type": "LLM",
    "batch_sizes": [1, 2, 4, 8, 16],
    "measurements": [
      {
        "batch_size": 1,
        "precision": "FP32",
        "inference_time_ms": 18.5,
        "memory_usage_mb": 14600.0,
        "throughput_samples_per_sec": 54.0,
        "gpu_utilization_percent": 85.0,
        "power_usage_watts": 320.0,
        "temperature_celsius": 65.0,
        "runs": 500,
        "std_dev_ms": 0.9
      },
      {
        "batch_size": 2,
        "precision": "FP16",
        "inference_time_ms": 28.2,
        "memory_usage_mb": 15000.0,
        "throughput_samples_per_sec": 70.9,
        "gpu_utilization_percent": 90.0,
        "power_usage_watts": 340.0,
        "temperature_celsius": 67.0,
        "runs": 500,
        "std_dev_ms": 1.2
      },
      {
        "batch_size": 4,
        "precision": "FP16",
        "inference_time_ms": 48.9,
        "memory_usage_mb": 15700.0,
        "throughput_samples_per_sec": 81.8,
        "gpu_utilization_percent": 93.0,
        "power_usage_watts": 360.0,
        "temperature_celsius": 69.0,
        "runs": 500,
        "std_dev_ms": 1.8
      },
      {
        "batch_size": 8,
        "precision": "FP16",
        "inference_time_ms": 89.3,
        "memory_usage_mb": 16600.0,
        "throughput_samples_per_sec": 89.6,
        "gpu_utilization_percent": 95.0,
        "power_usage_watts": 375.0,
        "temperature_celsius": 71.0,
        "runs": 500,
        "std_dev_ms": 2.5
      },
      {
        "batch_size": 16,
        "precision": "FP16",
        "inference_time_ms": 168.4,
        "memory_usage_mb": 18200.0,
        "throughput_samples_per_sec": 95.0,
        "gpu_utilization_percent": 97.0,
        "power_usage_watts": 390.0,
        "temperature_celsius": 73.0,
        "runs": 300,
        "std_dev_ms": 4.1
      }
    ],
    "system_info": {
      "driver_version": "495.29.05",
      "cuda_version": "11.8",
      "cpu_model": "Intel Xeon Platinum 8358",
      "memory_gb": 128.0,
      "pcie_generation": "PCIe 4.0"
    },
    "timestamp": "2024-11-15T16:45:00Z",
    "source": "LLaMA 2 A100 Official Benchmarks"
  },
  {
    "gpu_name": "RTX 4090",
    "gpu_architecture": "Ada Lovelace",
    "model_name": "LLaMA 2 7B",
    "model_type": "LLM",
    "batch_sizes": [1, 2, 4, 8, 16],
    "measurements": [
      {
        "batch_size": 1,
        "precision": "FP32",
        "inference_time_ms": 22.8,
        "memory_usage_mb": 14900.0,
        "throughput_samples_per_sec": 43.9,
        "gpu_utilization_percent": 82.0,
        "power_usage_watts": 280.0,
        "temperature_celsius": 62.0,
        "runs": 500,
        "std_dev_ms": 1.1
      },
      {
        "batch_size": 2,
        "precision": "FP16",
        "inference_time_ms": 33.6,
        "memory_usage_mb": 15300.0,
        "throughput_samples_per_sec": 59.5,
        "gpu_utilization_percent": 87.0,
        "power_usage_watts": 310.0,
        "temperature_celsius": 65.0,
        "runs": 500,
        "std_dev_ms": 1.4
      },
      {
        "batch_size": 4,
        "precision": "FP16",
        "inference_time_ms": 58.2,
        "memory_usage_mb": 16000.0,
        "throughput_samples_per_sec": 68.7,
        "gpu_utilization_percent": 91.0,
        "power_usage_watts": 340.0,
        "temperature_celsius": 68.0,
        "runs": 500,
        "std_dev_ms": 2.0
      },
      {
        "batch_size": 8,
        "precision": "FP16",
        "inference_time_ms": 108.5,
        "memory_usage_mb": 17200.0,
        "throughput_samples_per_sec": 73.7,
        "gpu_utilization_percent": 94.0,
        "power_usage_watts": 370.0,
        "temperature_celsius": 70.0,
        "runs": 500,
        "std_dev_ms": 3.2
      },
      {
        "batch_size": 16,
        "precision": "FP16",
        "inference_time_ms": 205.4,
        "memory_usage_mb": 19500.0,
        "throughput_samples_per_sec": 77.9,
        "gpu_utilization_percent": 96.0,
        "power_usage_watts": 395.0,
        "temperature_celsius": 72.0,
        "runs": 300,
        "std_dev_ms": 5.8
      }
    ],
    "system_info": {
      "driver_version": "537.13",
      "cuda_version": "12.1",
      "cpu_model": "Intel Core i9-13900K",
      "memory_gb": 32.0,
      "pcie_generation": "PCIe 4.0"
    },
    "timestamp": "2024-11-15T17:10:00Z",
    "source": "LLaMA 2 RTX 4090 Community Benchmarks"
  },
  {
    "gpu_name": "Tesla V100",
    "gpu_architecture": "Volta",
    "model_name": "ViT-Base/16",
    "model_type": "ViT",
    "batch_sizes": [1, 2, 4, 8],
    "measurements": [
      {
        "batch_size": 1,
        "precision": "FP32",
        "inference_time_ms": 28.6,
        "memory_usage_mb": 1800.0,
        "throughput_samples_per_sec": 35.0,
        "gpu_utilization_percent": 78.0,
        "power_usage_watts": 265.0,
        "temperature_celsius": 76.0,
        "runs": 500,
        "std_dev_ms": 1.2
      },
      {
        "batch_size": 2,
        "precision": "FP16",
        "inference_time_ms": 48.3,
        "memory_usage_mb": 2200.0,
        "throughput_samples_per_sec": 41.4,
        "gpu_utilization_percent": 85.0,
        "power_usage_watts": 280.0,
        "temperature_celsius": 78.0,
        "runs": 500,
        "std_dev_ms": 1.8
      },
      {
        "batch_size": 4,
        "precision": "FP16",
        "inference_time_ms": 92.7,
        "memory_usage_mb": 3000.0,
        "throughput_samples_per_sec": 43.1,
        "gpu_utilization_percent": 90.0,
        "power_usage_watts": 285.0,
        "temperature_celsius": 80.0,
        "runs": 500,
        "std_dev_ms": 2.9
      },
      {
        "batch_size": 8,
        "precision": "FP16",
        "inference_time_ms": 178.5,
        "memory_usage_mb": 4600.0,
        "throughput_samples_per_sec": 44.8,
        "gpu_utilization_percent": 92.0,
        "power_usage_watts": 290.0,
        "temperature_celsius": 81.0,
        "runs": 300,
        "std_dev_ms": 4.2
      }
    ],
    "system_info": {
      "driver_version": "460.32.03",
      "cuda_version": "11.2",
      "cpu_model": "Intel Xeon Silver 4216",
      "memory_gb": 64.0,
      "pcie_generation": "PCIe 3.0"
    },
    "timestamp": "2024-11-16T09:30:00Z",
    "source": "ViT Community Benchmarks"
  },
  {
    "gpu_name": "A100",
    "gpu_architecture": "Ampere",
    "model_name": "ViT-Base/16",
    "model_type": "ViT",
    "batch_sizes": [1, 2, 4, 8, 16],
    "measurements": [
      {
        "batch_size": 1,
        "precision": "FP32",
        "inference_time_ms": 12.4,
        "memory_usage_mb": 1600.0,
        "throughput_samples_per_sec": 80.6,
        "gpu_utilization_percent": 72.0,
        "power_usage_watts": 280.0,
        "temperature_celsius": 62.0,
        "runs": 500,
        "std_dev_ms": 0.6
      },
      {
        "batch_size": 2,
        "precision": "FP16",
        "inference_time_ms": 18.9,
        "memory_usage_mb": 2000.0,
        "throughput_samples_per_sec": 105.8,
        "gpu_utilization_percent": 78.0,
        "power_usage_watts": 310.0,
        "temperature_celsius": 64.0,
        "runs": 500,
        "std_dev_ms": 0.8
      },
      {
        "batch_size": 4,
        "precision": "FP16",
        "inference_time_ms": 32.7,
        "memory_usage_mb": 2800.0,
        "throughput_samples_per_sec": 122.3,
        "gpu_utilization_percent": 85.0,
        "power_usage_watts": 340.0,
        "temperature_celsius": 66.0,
        "runs": 500,
        "std_dev_ms": 1.2
      },
      {
        "batch_size": 8,
        "precision": "FP16",
        "inference_time_ms": 59.8,
        "memory_usage_mb": 4200.0,
        "throughput_samples_per_sec": 133.8,
        "gpu_utilization_percent": 89.0,
        "power_usage_watts": 360.0,
        "temperature_celsius": 68.0,
        "runs": 500,
        "std_dev_ms": 1.8
      },
      {
        "batch_size": 16,
        "precision": "FP16",
        "inference_time_ms": 113.8,
        "memory_usage_mb": 7000.0,
        "throughput_samples_per_sec": 140.6,
        "gpu_utilization_percent": 92.0,
        "power_usage_watts": 380.0,
        "temperature_celsius": 70.0,
        "runs": 300,
        "std_dev_ms": 2.8
      }
    ],
    "system_info": {
      "driver_version": "495.29.05",
      "cuda_version": "11.8",
      "cpu_model": "Intel Xeon Platinum 8358",
      "memory_gb": 128.0,
      "pcie_generation": "PCIe 4.0"
    },
    "timestamp": "2024-11-16T10:00:00Z",
    "source": "ViT A100 Official Benchmarks"
  },
  {
    "gpu_name": "RTX 4090",
    "gpu_architecture": "Ada Lovelace",
    "model_name": "ViT-Base/16",
    "model_type": "ViT",
    "batch_sizes": [1, 2, 4, 8, 16],
    "measurements": [
      {
        "batch_size": 1,
        "precision": "FP32",
        "inference_time_ms": 15.8,
        "memory_usage_mb": 1700.0,
        "throughput_samples_per_sec": 63.3,
        "gpu_utilization_percent": 68.0,
        "power_usage_watts": 250.0,
        "temperature_celsius": 58.0,
        "runs": 500,
        "std_dev_ms": 0.7
      },
      {
        "batch_size": 2,
        "precision": "FP16",
        "inference_time_ms": 23.2,
        "memory_usage_mb": 2100.0,
        "throughput_samples_per_sec": 86.2,
        "gpu_utilization_percent": 75.0,
        "power_usage_watts": 290.0,
        "temperature_celsius": 61.0,
        "runs": 500,
        "std_dev_ms": 0.9
      },
      {
        "batch_size": 4,
        "precision": "FP16",
        "inference_time_ms": 38.6,
        "memory_usage_mb": 2900.0,
        "throughput_samples_per_sec": 103.6,
        "gpu_utilization_percent": 82.0,
        "power_usage_watts": 320.0,
        "temperature_celsius": 64.0,
        "runs": 500,
        "std_dev_ms": 1.3
      },
      {
        "batch_size": 8,
        "precision": "FP16",
        "inference_time_ms": 69.4,
        "memory_usage_mb": 4500.0,
        "throughput_samples_per_sec": 115.3,
        "gpu_utilization_percent": 88.0,
        "power_usage_watts": 350.0,
        "temperature_celsius": 67.0,
        "runs": 500,
        "std_dev_ms": 2.1
      },
      {
        "batch_size": 16,
        "precision": "FP16",
        "inference_time_ms": 128.9,
        "memory_usage_mb": 7400.0,
        "throughput_samples_per_sec": 124.1,
        "gpu_utilization_percent": 91.0,
        "power_usage_watts": 375.0,
        "temperature_celsius": 69.0,
        "runs": 300,
        "std_dev_ms": 3.2
      }
    ],
    "system_info": {
      "driver_version": "537.13",
      "cuda_version": "12.1",
      "cpu_model": "Intel Core i9-13900K",
      "memory_gb": 32.0,
      "pcie_generation": "PCIe 4.0"
    },
    "timestamp": "2024-11-16T10:30:00Z",
    "source": "ViT RTX 4090 Community Benchmarks"
  },
  {
    "gpu_name": "RTX 4090",
    "gpu_architecture": "Ada Lovelace", 
    "model_name": "YOLO v8",
    "model_type": "CNN",
    "batch_sizes": [2, 12, 20, 24, 32],
    "measurements": [
      {
        "batch_size": 2,
        "precision": "FP32",
        "inference_time_ms": 12.8,
        "memory_usage_mb": 2400.0,
        "throughput_samples_per_sec": 156.3,
        "gpu_utilization_percent": 72.0,
        "power_usage_watts": 280.0,
        "temperature_celsius": 59.0,
        "runs": 500,
        "std_dev_ms": 0.9
      },
      {
        "batch_size": 12,
        "precision": "FP16",
        "inference_time_ms": 58.4,
        "memory_usage_mb": 7200.0,
        "throughput_samples_per_sec": 205.5,
        "gpu_utilization_percent": 91.0,
        "power_usage_watts": 365.0,
        "temperature_celsius": 68.0,
        "runs": 500,
        "std_dev_ms": 2.4
      },
      {
        "batch_size": 20,
        "precision": "FP16",
        "inference_time_ms": 95.2,
        "memory_usage_mb": 11800.0,
        "throughput_samples_per_sec": 210.1,
        "gpu_utilization_percent": 95.0,
        "power_usage_watts": 385.0,
        "temperature_celsius": 71.0,
        "runs": 300,
        "std_dev_ms": 3.8
      },
      {
        "batch_size": 24,
        "precision": "FP16",
        "inference_time_ms": 112.6,
        "memory_usage_mb": 14000.0,
        "throughput_samples_per_sec": 213.2,
        "gpu_utilization_percent": 96.0,
        "power_usage_watts": 390.0,
        "temperature_celsius": 72.0,
        "runs": 300,
        "std_dev_ms": 4.2
      },
      {
        "batch_size": 32,
        "precision": "FP16",
        "inference_time_ms": 148.9,
        "memory_usage_mb": 18200.0,
        "throughput_samples_per_sec": 214.8,
        "gpu_utilization_percent": 97.0,
        "power_usage_watts": 395.0,
        "temperature_celsius": 74.0,
        "runs": 200,
        "std_dev_ms": 5.6
      }
    ],
    "system_info": {
      "driver_version": "537.13",
      "cuda_version": "12.1",
      "cpu_model": "Intel Core i9-13900K",
      "memory_gb": 32.0,
      "pcie_generation": "PCIe 4.0"
    },
    "timestamp": "2024-11-16T14:00:00Z",
    "source": "Extended YOLOv8 RTX 4090 Validation Dataset"
  }
] 